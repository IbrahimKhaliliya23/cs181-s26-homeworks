\documentclass[submit]{harvardml}
\usepackage{common}

\course{CS1810-S26}
\assignment{Homework \#1}
\duedate{February 13, 2026 at 11:59 PM}
\usepackage{float}
\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{framed}
\usepackage{float}
\usepackage{ifthen}
\usepackage{bm}


\usepackage[mmddyyyy,hhmmss]{datetime}



\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

 \DeclareMathOperator*{\limover}{\overline{lim}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Solution environment
\usepackage{xcolor}
\newenvironment{solution}{
    \vspace{2mm}
    \color{blue}\noindent\textbf{Solution}:
}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{center}
  {\Large Regression}
\end{center}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[kNN and Kernels, 35pts]

You will now implement two non-parametric regressions to model temperatures over time.  
% For this problem, you will use the \textbf{same dataset as in Problem 1}.

\vspace{0.5cm}
\noindent\emph{Make sure to include all required plots in your PDF. Passing all test cases does not guarantee that your solution is correct, and we encourage you to write your own. }

\begin{enumerate}
\item 
 Recall that kNN uses a predictor of the form
\[
  f(x^*) = \frac{1}{k} \sum_n y_n \mathbb{I}(x_n \texttt{ is one of k-closest to } x^*),
\]
where $\mathbb{I}$ is an indicator variable. 
\begin{enumerate}

  \item The kNN implementation \textbf{has been provided for you} in the notebook. Run the cells to plot the results for $k=\{1, 3, N-1\}$, where $N$ is the size of the dataset. Describe how the fits change with $k$. Please include your plot in your solution PDF.

  \item Now, we will evaluate the quality of each model \emph{quantitatively} by computing the error on the provided test set. Write Python code to compute the test MSE for each value of $k$.  Report the values here. Which solution has the lowest MSE? 
  
\end{enumerate}

\item \textit{Kernel-based regression} techniques are another form of non-parametric regression. Consider a kernel-based
regressor of the form 
\begin{equation*}
  f_\tau(x^*) = \cfrac{\sum_{n} K_\tau(x_n,x^*) y_n}{\sum_n K_\tau(x_n, x^*)}
\end{equation*}
where $\mathcal{D}_\texttt{train} = \{(x_n,y_n)\}_{n = 1} ^N$ are the
training data points, and $x^*$ is the point for which you want to
make the prediction.  The kernel $K_\tau(x,x')$ is a function that
defines the similarity between two inputs $x$ and $x'$. A popular
choice of kernel is a function that decays as the distance between the
two points increases, such as
\begin{equation*}
  K_\tau(x,x') = \exp\left(-\frac{(x-x')^2}{\tau}\right)
\end{equation*}

where $\tau$ represents the square of the lengthscale (a scalar value that
dictates how quickly the kernel decays).  


\begin{enumerate}
    
  \item First, implement the \texttt{kernel\_regressor} function in the notebook, and plot your model for years in the range $800,000$ BC to $400,000$ BC at $1000$ year intervals for the following three values of $\tau$: $1, 50, 2500$. Since we're working in terms of thousands of years, this means you should plot $(x, f_\tau(x))$ for $x = 400, 401, \dots, 800$. \textbf{In no more than 10 lines}, describe how the fits change with $\tau$. Please include your plot in your solution PDF.

  \item Denote the test set as $\mathcal{D}_\texttt{test} = \{(x'_m, y'_m)\}_{m = 1} ^M$.  Write down the expression for the MSE of $f_\tau$ over the test set as a function of the training set and test set. Your answer may include $\{(x'_m, y'_m)\}_{m = 1} ^M$, $\{(x_n, y_n)\}_{n = 1} ^N$, and $K_\tau$, but not $f_\tau$.

    \item Compute the MSE on the provided test set for the three values of $\tau$.  Report the values here. Which model yields the lowest MSE? Conceptually, why is this the case? Why would choosing $\tau$ based on $\mathcal{D}_\texttt{train}$ rather than $\mathcal{D}_\texttt{test}$ be a bad idea? 

  \item Describe the time and space complexity of both kernelized regression and kNN with respect to the size of the training set $N$.  How, if at all, does the size of the model---everything that needs to be stored to make predictions---change with the size of the training set $N$?  How, if at all, do the number of computations required to make a prediction for some input $x^*$ change with the size of the training set $N$?.
  

  \item  What is the exact form of $\lim_{\tau \to 0 }f_\tau(x^*)$?
  \end{enumerate}
\end{enumerate}
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item SubProblem 1
        \begin{enumerate}
            \item Functions plot for $K=1,3,N-1$.\medskip
            
            For $K=1$, the fit indicates the model is predicting the training data almost perfectly, since each prediction is the value of the nearest training point, which means that the model has low bias and high variance. The model might not generalize well to unseen data.\medskip

            As for $K=3$, the fit is smoother than $K=1$ since we're averaging the values of the nearest 3 training points, which slightly reduces variance and increases bias.\medskip

            Lastly, for $K=N-1$, this is the extreme opposite of $K=1$, where fit indicates the model is averaging almost all values of training points, causing every prediction to be the global average of all points. The model has high bias and low variance.
            {
            \begin{figure}[H]
            \centering
            \includegraphics[width=0.38\textwidth]{p1.1a.png}
            \end{figure}}
            \item $K=1$, $mse=$  $1.7406000000000004$
    $K=3$, $mse=$  $3.8907662222222212$
                    $K=N-1$, $mse=$ $9.528571442602042$.
                    Although the model with $K=1$ is theoretically high variance and low bias. From our MSE calculation it appears that the the dataset is relatively low-noise and therefore the model has lowest $mse$.
        \end{enumerate}
        \item SubProblem 2 
        \begin{enumerate}
            \item Based on the the different 3 fits it appears that $\tau$ controls how broadly are we assigning to nearby points. For $\tau =1$ we can see that the nearest points get most weight and that's why we have a non-smooth fit since each prediction is heavily influenced by the closes training point. As we increase $\tau$ to $50$ More nearby points contribute to the prediction. Lastly we can notice when we increase $\tau$ to $2500$ the fit becomes smother and the model appears to have more bias since many more points recieve similar weight.
            \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{p1.2a.png}
            \end{figure}
            \item For each test input $x'_m$ the kernel's predictor is
            
            \[
\hat{y}'_m \;=\;
\frac{\sum_{n=1}^{N} K_{\tau}(x_n, x'_m)\, y_n}
{\sum_{n=1}^{N} K_{\tau}(x_n, x'_m)}
\]

So the $mse$ over test is:
\[
\mathrm{MSE}(\tau)
\;=\;
\frac{1}{M}\sum_{m=1}^{M}
\left(
y'_m
-
\frac{\sum_{n=1}^{N} K_{\tau}(x_n, x'_m)\, y_n}
{\sum_{n=1}^{N} K_{\tau}(x_n, x'_m)}
\right)^2
\]

        \item $\tau=1$ : $mse= 1.9472621565209178$
        \smallskip
        
        $\tau=50$ : $mse= 1.858289916961345$ $\rightarrow  $ lowest $mse$
        \smallskip
        
        $\tau=2500$ : $mse = 8.333886806980791$

        This is because because $\tau= 50$ is the best bias-variance tradeoff compared $\tau=1$ (high variance), and $\tau=2500$ (high bias).
        \smallskip

        choosing $\tau$ based on $\mathcal{D}_\texttt{train}$ rather than $\mathcal{D}_\texttt{test}$ is a bad idea because that would be tuning $\tau$ to do well on the data we already fit on, which increases overfitting. More specifically, training error favors small $\tau$ since it decreases the $mse$ even if that means the model generalizes badly.
        \item Both kNN and kernel regression store the entire training dataset 
$\{(x_n, y_n)\}_{n=1}^{N}$. Therefore, the space complexity of both models 
is $\mathrm{}{O}(N)$. For kNN, making a prediction at a new point $x^*$ requires computing the 
distance from $x^*$ to all $N$ training points, which takes 
$\mathrm{O}(N)$ time. Selecting the $k$ nearest neighbors requires either 
sorting ($\mathrm{O}(N \log N)$)
($\mathrm{O}(N)$). Thus, the overall prediction time is 
$\mathrm{O}(N)$ (or $\mathrm{O}(N \log N)$ with  sorting). For kernel regression, making a prediction at $x^*$ requires computing 
the kernel value $K_\tau(x_n, x^*)$ for all $N$ training points and 
computing the weighted numerator and denominator sums. This requires 
$\mathrm{O}(N)$ time. The two models' sizes grows linearly with $N$, and the number of 
computations required to make a single prediction also grows linearly 
with $N$.

\item Recall
\[
f_\tau(x^*)
=
\frac{\sum_{n=1}^{N} 
\exp\!\left(-\frac{(x_n - x^*)^2}{\tau}\right) y_n}
{\sum_{n=1}^{N} 
\exp\!\left(-\frac{(x_n - x^*)^2}{\tau}\right)}.
\]

Let
\[
d_n = (x_n - x^*)^2.
\]

Then
\[
f_\tau(x^*)
=
\frac{\sum_{n=1}^{N} e^{-d_n/\tau} y_n}
{\sum_{n=1}^{N} e^{-d_n/\tau}}.
\]

As $\tau \to 0$:

- If $d_n > 0$, then $e^{-d_n/\tau} \to 0$.
- The smallest $d_n$ dominates the sum.

Let
\[
n^* = \arg\min_n (x_n - x^*)^2.
\]

Then the numerator and denominator are dominated by the $n^*$ term:

\[
\sum_{n=1}^{N} e^{-d_n/\tau} y_n 
\sim e^{-d_{n^*}/\tau} y_{n^*},
\]
\[
\sum_{n=1}^{N} e^{-d_n/\tau}
\sim e^{-d_{n^*}/\tau}.
\]

Therefore,
\[
\lim_{\tau \to 0} f_\tau(x^*)
=
y_{n^*}.
\]

        \end{enumerate}
	\end{enumerate}
\end{solution}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Geometric Least Squares, 20pts]
    Linear regression can be understood using geometric intuition in $\mathbb{R}^n$. The design matrix $\mathbf X \in \mathbb{R}^{N\times D}$ spans a subspace $C(\mathbf X)$, the column space of $\mathbf X$ (referred to in lecture as column span) which is a subspace of $\mathbb{R}^N$. If you wish to review the concept of a column space, consider visiting Section 0 notes. \\
    
    \noindent Fitting by linear regression, sometimes called \textit{ordinary least-squares} (OLS), is just projecting the observation vector $\mathbf y \in \mathbb{R}^N$ orthogonally onto that subspace. Lecture 2 slides provide a good graphic to visualize this, see the slide titled ``Geometric Interpretation.'' From lecture, we also learned that $\hat {\mathbf y}$ lives in $C(\mathbf X)$ and the residual $\mathbf r = {\mathbf y} - \hat {\mathbf y}$ lives in the orthogonal complement.

    \begin{enumerate}
    \item Let $\mathbf X \in \mathbb{R}^{N\times D}$ have full column rank $d$ and $\mathbf y \in \mathbb{R}^N$. Let the OLS estimator be $\mathbf w^*=(\mathbf X^\top \mathbf X)^{-1} X^\top \mathbf y$ and the fitted vector $\mathbf y = \mathbf X \mathbf w^*$. Prove that $\hat {\mathbf y}$ is the \textit{orthogonal projection} of $y$ onto $C(\mathbf X)$. In other words, show that $\hat {\mathbf y} \in C(\mathbf X)$ and $\mathbf y - \hat {\mathbf y}$ is orthogonal to $C(\mathbf X)$. \textit{Hint: To show orthogonality, look at the gradient of $\mathcal L$, the loss, with respect to $\mathbf w$}.

    \item Prove that among all vectors in $C(\mathbf X)$, the fitted vector $\hat {\mathbf y}$ minimizes the Euclidean distance to $\mathbf y$. In other words, that for every vector $\mathbf v \in C(\mathbf X)$:
    \begin{equation*}
        \|\mathbf y - \hat {\mathbf y}\|_2^2 \leq \|\mathbf y - \mathbf v\|_2
    \end{equation*}
    Looking back at lecture, this is the formal proof of the phenomenon discussed in the image. \textit{Hint: For two vectors, $\mathbf v,\mathbf w$, if $\mathbf v$ is orthogonal to $\mathbf w$, denoted as $\mathbf v \perp \mathbf w$, then $\|\mathbf v-\mathbf w\|^2_2 = \|\mathbf v\|_2^2+\|\mathbf w\|^2_2$ (Pythagorean theorem).}

    \item In lecture, we defined the projection matrix, $\mathbf P = \mathbf X(\mathbf X^\top \mathbf X)^{-1} \mathbf X^\top$, which projects onto the subspace $C(\boldX)$. The matrix $\mathbf P$ is often called the \textit{hat matrix} because it maps $\mathbf y$ to its fitted values $\hat {\mathbf y} = \mathbf P \mathbf y$, i.e., it ``puts a hat" on $\mathbf y$. Prove the following properties of $\mathbf P$:
    \begin{itemize}
        \item Symmetry: $\mathbf P^\top = \mathbf P$
        \item Idempotence: $\mathbf P^2 = \mathbf P$
        \item Rank and Trace: $\mathrm{rank}(\mathbf P) = d$ and $\mathrm{trace}(\mathbf P) = d$.
    \end{itemize}
    % Also, provide geometric interpretation of the first two properties. 
    \textit{Hint: You may use the fact that any idempotent matrix has equal rank and trace. You do not need to prove this, but it may be helpful to think about why this is true.}
    
    \item Suppose you fit your model as in Problem 5.1. You observe that the \textbf{residual plot} exhibits a clear parabolic (U-shaped) pattern rather than random scatter around zero (as seen in lecture). Give a geometric interpretation of this phenomenon in terms of projection onto the column space of the design matrix. Also, explain how adding a quadratic basis function affects the geometry of the regression problem and the residuals.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.7]{img_input/residual_plot.png}
            \caption{An example residual plot with the input variable $x$ on the horizontal axis and residuals $y-\hat{y}$ on the vertical axis.}

          \end{figure}
    \end{enumerate}
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item Subproblem 1
        \smallskip

        Let the squared loss be
\[
\mathcal{L}(\mathbf w)=\frac{1}{2}\|\mathbf y-\mathbf X\mathbf w\|_2^2.
\]
The fitted vector is $\hat{\mathbf y}=\mathbf X\mathbf w^*$ where
\[
\mathbf w^*=(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf y.
\]

 $\hat{\mathbf y}\in C(\mathbf X)$.
By definition, $\hat{\mathbf y}=\mathbf X\mathbf w^*$ is a linear combination of the columns of $\mathbf X$,
so $\hat{\mathbf y}\in C(\mathbf X)$.

 $\mathbf y-\hat{\mathbf y}\perp C(\mathbf X)$.
Compute the gradient:
\[
\nabla_{\mathbf w}\mathcal{L}(\mathbf w)
= -\mathbf X^\top(\mathbf y-\mathbf X\mathbf w).
\]
At the minimizer $\mathbf w^*$ we have $\nabla_{\mathbf w}\mathcal{L}(\mathbf w^*)=0$, hence
\[
\mathbf X^\top(\mathbf y-\mathbf X\mathbf w^*)=\mathbf 0
\quad\Longrightarrow\quad
\mathbf X^\top(\mathbf y-\hat{\mathbf y})=\mathbf 0.
\]
Let $\mathbf v\in C(\mathbf X)$. Then $\mathbf v=\mathbf X\mathbf a$ for some $\mathbf a\in\mathbb R^D$, and
\[
\mathbf v^\top(\mathbf y-\hat{\mathbf y})
=(\mathbf X\mathbf a)^\top(\mathbf y-\hat{\mathbf y})
=\mathbf a^\top \mathbf X^\top(\mathbf y-\hat{\mathbf y})
=\mathbf a^\top \mathbf 0
=0.
\]
Thus $(\mathbf y-\hat{\mathbf y})$ is orthogonal to every vector in $C(\mathbf X)$, i.e.
\[
\mathbf y-\hat{\mathbf y}\perp C(\mathbf X).
\]

Since $\hat{\mathbf y}\in C(\mathbf X)$ and $\mathbf y-\hat{\mathbf y}\perp C(\mathbf X)$, $\hat{\mathbf y}$ is
the orthogonal projection of $\mathbf y$ onto $C(\mathbf X)$.

\item SubProblem 2

\smallskip

Let $\hat{\mathbf y}$ be the orthogonal projection of $\mathbf y$ onto $C(\mathbf X)$, so
$\hat{\mathbf y}\in C(\mathbf X)$ and $\mathbf r:=\mathbf y-\hat{\mathbf y}\perp C(\mathbf X)$.

Take any $\mathbf v\in C(\mathbf X)$. Then $\mathbf v-\hat{\mathbf y}\in C(\mathbf X)$ as well, hence
\[
\mathbf r \perp (\mathbf v-\hat{\mathbf y}).
\]
Now decompose the error to $\mathbf v$ as
\[
\mathbf y-\mathbf v
=
(\mathbf y-\hat{\mathbf y})+(\hat{\mathbf y}-\mathbf v)
=
\mathbf r-(\mathbf v-\hat{\mathbf y}).
\]
By the Pythagorean theorem (since $\mathbf r \perp (\mathbf v-\hat{\mathbf y})$),
\[
\|\mathbf y-\mathbf v\|_2^2
=
\|\mathbf r\|_2^2+\|\mathbf v-\hat{\mathbf y}\|_2^2
\ge
\|\mathbf r\|_2^2
=
\|\mathbf y-\hat{\mathbf y}\|_2^2.
\]
Therefore, for every $\mathbf v\in C(\mathbf X)$,
\[
\|\mathbf y-\hat{\mathbf y}\|_2^2 \le \|\mathbf y-\mathbf v\|_2^2,
\]
so $\hat{\mathbf y}$ minimizes the Euclidean distance from $\mathbf y$ among all vectors in $C(\mathbf X)$.

\item SubProblem 3

Let $\mathbf X\in\mathbb R^{N\times D}$ have full column rank $d$ and define the (hat) projection matrix
\[
\mathbf P=\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top.
\] 

\textbf{Symmetry.}
Using $(\mathbf X^\top\mathbf X)^{-1}$ is symmetric,
\[
\mathbf P^\top
=\left(\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top\right)^\top
=\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top
=\mathbf P.
\]

\textbf{Idempotence.}
\[
\mathbf P^2
=\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top
\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top
=\mathbf X(\mathbf X^\top \mathbf X)^{-1}(\mathbf X^\top\mathbf X)(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top
=\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top
=\mathbf P.
\]

\textbf{}{Rank.}
For any $\mathbf a\in\mathbb R^D$,
\[
\mathbf P(\mathbf X\mathbf a)=\mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf X\mathbf a
=\mathbf X\mathbf a,
\]
so $\mathrm{Im}(\mathbf P)=C(\mathbf X)$. Hence
\[
\mathrm{rank}(\mathbf P)=\dim(\mathrm{Im}(\mathbf P))=\dim(C(\mathbf X))=d.
\]

\textbf{Trace.}
Since $\mathbf P$ is idempotent ($\mathbf P^2=\mathbf P$), we may use the fact that any idempotent
matrix satisfies $\mathrm{trace}(\mathbf P)=\mathrm{rank}(\mathbf P)$. Therefore,
\[
\mathrm{trace}(\mathbf P)=\mathrm{rank}(\mathbf P)=d.
\]
\item SubProblem 4

 U-shaped residual plot means the linear model is missing curvature.
Geometrically, OLS projects $\mathbf y$ onto the column space $C(\mathbf X)$.
If $C(\mathbf X)$ only contains linear functions, it cannot
capture a quadratic trend in the data. The leftover quadratic component stays
in the residual vector, which is why the residuals show a systematic parabolic
pattern instead of random scatter. Adding a quadratic basis function ($x^2$) expands the column space to include
that curved direction. Now the projection can capture the parabola, and the
residuals become more random around zero because the  structure has
been explained and conveyed by the model
	\end{enumerate}
\end{solution}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Basis Regression, 30pts]
    We now implement some linear regression models for the temperature. If we just directly use the data as given to us, we would only have a one dimensional input to our model, the year.  To create a more expressive linear model, we will introduce basis functions.
    
    \vspace{1em}
    
    \noindent\emph{Make sure to include all required plots in your PDF.}
    
    \begin{enumerate}
        \item We will first implement the four basis regressions below. Note that we introduce an addition transform $f$ (already into the provided notebook) to address concerns about numerical instabilities.
        
        \begin{enumerate}
            \item $\phi_j(x)= f(x)^j$ for $j=1,\ldots, 9$. $f(x) = \frac{x}{1.81 \cdot 10^{2}}.$
          
            \item $\phi_j(x) = \exp\left\{-\cfrac{(f(x)-\mu_j)^2}{5}\right\}$ for $\mu_j=\frac{j + 7}{8}$ with $j=1,\ldots, 9$. $f(x) = \frac{x}{4.00 \cdot 10^{2}}.$
          
            \item $\phi_j(x) =  \cos(f(x) / j)$ for $j=1, \ldots, 9$. $f(x) = \frac{x}{1.81}$.
          
            \item $\phi_j(x) = \cos(f(x) / j)$ for $j=1, \ldots, 49$. $f(x) = \frac{x}{1.81 \cdot 10^{-1}}$. \footnote{For the trigonometric bases (c) and (d), the periodic nature of cosine requires us to transform the data such that the lengthscale is within the periods of each element of our basis.}
        \end{enumerate}
    
        {\footnotesize *Note: Please make sure to add a bias term for all your basis functions above in your implementation of the \verb|make_basis|.}
    
        Let
        $$ \mathbf{\phi}(\mathbf{X}) = \begin{bmatrix}
            \mathbf{\phi}(x_1) \\
            \mathbf{\phi}(x_2) \\
            \vdots             \\
            \mathbf{\phi}(x_N) \\
        \end{bmatrix} \in \mathbb{R}^{N\times D}. $$
        You will complete the \verb|make_basis| function which must return $\phi(\mathbf{X})$ for each part (a) - (d). You do NOT need to submit this code in your \LaTeX writeup.
    
        Then, create a plot of the fitted regression line for each basis against a scatter plot of the training data. Boilerplate plotting code is provided in the notebook---you will only need to finish up a part of it. \textbf{All you need to include in your writeup for this part are these four plots.}
    
        \item Now we have trained each of our basis regressions. For each basis regression, compute the MSE on the test set. Discuss: do any of the bases seem to overfit? Underfit? Why?
    
        \item Briefly describe what purpose the transforms $\phi$ serve: why are they helpful?
    
        \item As in Problem 1, describe the space and time complexity of linear regression.  How does what is stored to compute predictions change with the size of the training set $N$ and the number of features $D$?  How does the computation needed to compute the prediction for a new input depend on the size of the training set $N$?  How do these complexities compare to those of the kNN and kernelized regressor?
    
        \item Briefly compare and constrast the different regressors: kNN, kernelized regression, and linear regression (with bases). Are some regressions clearly worse than others?  Is there one best regression?  How would you use the fact that you have these multiple regression functions?
    \end{enumerate}
      
    \noindent \textit{Note:} You may be concerned that we are using a different set of inputs $\mathbf{X}$ for each basis (a)-(d), since it could seem as though this prevents us from being able to directly compare the MSE of the models since we are using different data as input. But this is not an issue, since each transformation is considered as being a part of our model. This contrasts with transformations that cause the variance of the target $\mathbf{y}$ to be different  (such as standardization); in these cases the MSE can no longer be directly compared.
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item SubProblem 1
         \begin{figure}[H]
            \centering
            \includegraphics[width=0.6\textwidth]{p3.1.png}
            \end{figure}
        \item SubProblem 2
        \begin{enumerate}
            \item a : $mse =7.955749288900331$
            \item b : $mse = 8.708150363771939$
            \item c : $mse = 5.967024534016803$
            \item d : $mse = 58.89042789823659$ $\rightarrow$ This model appears to be overfitting, we only have 57 features in our training points and this model has 49 cosine features, enabling the fit to oscillate heavily between points, causing high variance and low bias. (In math words, the projection space is large enough to interpolate noise)
        \end{enumerate}
        \item he transforms $\phi$ allow us to turn a simple linear model into a much more expressive model.
Instead of fitting a straight line in the original input space, we map the input into a higher-dimensional feature space where nonlinear relationships (polynomial trends, periodic behavior, etc.) can be represented.

\item Computing $\mathbf w^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf y$
requires building $\mathbf X^\top \mathbf X$ in $O(ND^2)$ time and inverting a
$D \times D$ matrix in $O(D^3)$ time. After training, we only store $\mathbf w \in \mathbb R^D$, so the model size is $O(D)$.
It does \emph{not} grow with $N$. To make a prediction $x^*$, we compute $\hat y = \mathbf w^\top \phi(x^*)$,
which costs $O(D)$. Prediction does not depend on $N$. Linear regression has higher training cost but very fast prediction and fixed model size.
kNN and kernel regression have almost negligible training cost, but prediction is expensive
and grows linearly with $N$. As $N$ grows, linear regression becomes much more
efficient at testing.

\item kNN, kernel regression, and linear regression (with bases) differ in how they control model
complexity and how they generalize. kNN and kernel regression are non-parametric methods. They store the entire training
dataset and make predictions based on local neighborhoods. They are flexible and can adapt to complex patterns(as discussed in lecture), but prediction time scales with $N$, and they can easily overfit if the neighborhood is too small (small $k$ or small $\tau$). Linear regression with basis functions is parametric. After training, it only stores
the weight vector $\mathbf w$, so prediction is fast and independent of $N$. The choice of basis controls model flexibility. With too few basis functions the model
underfits; with too many it overfits. No regression method is universally best. The best model depends on the data and the
biasâ€“variance tradeoff. 
	\end{enumerate}
\end{solution}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Probablistic View of Regression and Regularization, 30pts]
    Finally, we will explore an alternative view of linear regression to what was introduced in lecture. This view will be probabilistic. We will also introduce Bayesian regression under this probabilistic view. We will explore its connection to regularization for linear models, and then fit a regularized model to the temperature data. The probabilistic interpretation of linear regression is explored in more detail in the \href{https://github.com/harvard-ml-courses/cs181-textbook/blob/master/Textbook.pdf}{course notes} under section 2.6.2, but we have also tried to make this question self-contained with all necessary content.
    \\
    
    \noindent Recall that linear regression involves having $N$ labeled data points, say, $(\boldx_n,y_n)$ for $n\in\{1,\dots,N\}$. A probabilistic view of the linear regression problem supposes that the data actually came from a probabilistic model:
    \[y_n = \boldw^\top\boldx_n + \epsilon_n, \quad \epsilon_n \sim \mathcal{N}(0, \sigma^2).\]
    That is, we assume that there exists a set of coefficients $\boldw$ such that given data $\boldx_n$, the corresponding $y_n$ results from taking the $\boldw^\top\boldx_n$ and adding some random noise $\epsilon_n$. Here, we assume the noise is normally distributed with known mean and variance. The introduction of noise into the model accounts for the possibility of scatter, i.e., when the data does not literally follow a perfect line. It is shown in the aforementioned section of the course notes that under this probabilistic model, the data likelihood $p(\boldy|\boldw,\boldX)$ is maximized by $\boldw^* = (\boldX^\top\boldX)^{-1} \boldX^\top \boldy$, which, as we already saw in class, also minimizes the squared error. So, amazingly, the probabilistic view of regression leads to the view we saw in lecture, where we are trying to minimize a prediction error. \\
    
    \noindent Now, Bayesian regression takes this probablistic view a step further. You may recall that Bayesian statistics involves choosing a prior distribution for the parameters, here $\boldw$, based on our prior beliefs. So, in Bayesian regression, we additionally assume the weights are distributed $p(\boldw)$ and fit the weights $\boldw$ by maximizing the posterior likelihood
    \[ p(\boldw | \boldX, \boldy) = \frac{p(\bold y | \boldw, \boldX)p(\boldw)}{p(\boldy | \boldX)}. \]
    Note that since we maximize with respect to $\boldw$, it suffices to just maximize the numerator, while the denominator term does not need to be computed.
    
    \begin{enumerate}
        \item Suppose $\boldw \sim \mathcal{N}(\mathbf{0},\frac{\sigma^2}{\lambda}\boldI)$. Show that maximizing the posterior likelihood is equivalent to minimizing the loss function
        \[\mathcal{L}_{ridge}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2 + \frac{\lambda}{2}||\boldw||_2^2.\] 
        For those who are familiar, note that minimizing $\mathcal{L}_{ridge}(\boldw)$ is exactly what regression with ridge regularization does.
        
        \textit{Hint:} You don't need to explicitly solve for the form of the maximizer/minimizer to show that the optimization problems are equivalent.
        
        \item Solve for the value of $\boldw$ that minimizes $\mathcal L_{ridge}(\boldw)$.
    
        \item The Laplace distribution has the PDF
       \[L(a,b) =\frac{1}{2b} \exp\left(-\frac{|x - a|}{b}\right)\]
        Show that if all $w_d \sim L\left(0,\frac{2\sigma^2}{\lambda}\right)$, maximizing the posterior likelihood is equivalent to minimizing the loss function
        \[\mathcal{L}_{lasso}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2  + \frac{\lambda}{2}||\boldw||_1.\] 
        For those who are familiar, note that minimizing $\mathcal{L}_{lasso}(\boldw)$ is exactly what regression with LASSO regularization does.
    
        \item The LASSO estimator is the value of $\boldw$ that minimizes $\mathcal{L}_{lasso}(\boldw)$? It is very useful in certain real-world scenarios. Why is there no general closed form for the LASSO estimator?
    
        \item Since there is no general closed form for the LASSO estimator $\boldw$, we use numerical methods for estimating $\boldw$. One approach is to use \textit{coordinate descent}, which works as follows: 
        \begin{enumerate}
            \item Initialize $\boldw=\boldw_0$.
            \item For each $d=1, \ldots, D$ do the following 2 steps consecutively:
            \begin{enumerate}
                \item Compute $\rho_d = \tilde{\boldx}_d^\top(\boldy - (\boldX \boldw - w_d \tilde{\boldx}_d))$. We define $\tilde{\boldx}_d$ as the $d$-th column of $\boldX$.
    
                \item If $d=1$, set $w_1 = \frac{\rho_1}{||\tilde{\boldx}_1||^2_2}$. Otherwise if $d\ne 1$, compute $w_d = \frac{\text{sign}(\rho_d)\max\left\{|\rho_d|-\frac{\lambda}{2}, 0\right\}}{||\tilde{\boldx}_d||^2_2}$.
            \end{enumerate}
            \item Repeat step (b) until convergence or the maximum number of iterations is reached.
        \end{enumerate} 
    
        Implement the \texttt{find\_lasso\_weights} function according to the above algorithm, letting $\boldw_0$ be a vector of ones and the max number of iterations be 5000. Then, fit models with $\lambda=1, 10$ to basis (d) from Problem 3 and plot the predictions on the train set. Finally, compute the test MSE's. You will need to do some preprocessing, but a completed helper function for this is already provided. How do the graphs and errors compare to those for the unregularized (i.e., vanilla) basis (d) model? 
    \end{enumerate}
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item SubProblem 1

By Bayes' rule,
\[
p(\mathbf w\mid \mathbf X,\mathbf y)\ \propto\ p(\mathbf y\mid \mathbf w,\mathbf X)\,p(\mathbf w).
\]
Thus maximizing the posterior is equivalent to maximizing the log-posterior:
\[
\arg\max_{\mathbf w}\log p(\mathbf w\mid \mathbf X,\mathbf y)
=
\arg\max_{\mathbf w}\left[\log p(\mathbf y\mid \mathbf w,\mathbf X)+\log p(\mathbf w)\right].
\]

\textbf{Likelihood.}
From $y_n=\mathbf w^\top \mathbf x_n+\epsilon_n$ with $\epsilon_n\sim\mathcal N(0,\sigma^2)$,
\[
p(\mathbf y\mid \mathbf w,\mathbf X)=\mathcal N(\mathbf X\mathbf w,\sigma^2\mathbf I),
\]
so (dropping constants independent of $\mathbf w$)
\[
\log p(\mathbf y\mid \mathbf w,\mathbf X)
= -\frac{1}{2\sigma^2}\|\mathbf y-\mathbf X\mathbf w\|_2^2 + C.
\]

\textbf{Prior.}
Given $\mathbf w\sim \mathcal N\!\left(\mathbf 0,\frac{\sigma^2}{\lambda}\mathbf I\right)$,
\[
\log p(\mathbf w)
= -\frac{\lambda}{2\sigma^2}\|\mathbf w\|_2^2 + C'.
\]

\textbf{Combine.}
Therefore,
\[
\log p(\mathbf y\mid \mathbf w,\mathbf X)+\log p(\mathbf w)
=
-\frac{1}{2\sigma^2}\|\mathbf y-\mathbf X\mathbf w\|_2^2
-\frac{\lambda}{2\sigma^2}\|\mathbf w\|_2^2
+ \text{const}.
\]
Maximizing this is equivalent to minimizing its negative, and multiplying by $\sigma^2$ does not change
the minimizer. Hence,
\[
\arg\max_{\mathbf w} p(\mathbf w\mid \mathbf X,\mathbf y)
=
\arg\min_{\mathbf w}\left(
\frac{1}{2}\|\mathbf y-\mathbf X\mathbf w\|_2^2
+\frac{\lambda}{2}\|\mathbf w\|_2^2
\right)
=
\arg\min_{\mathbf w}\ \mathcal L_{\text{ridge}}(\mathbf w).
\]

\item SubProblem 2 

Recall the ridge loss:
\[
\mathcal L_{\text{ridge}}(\mathbf w)
=
\frac{1}{2}\|\mathbf y-\mathbf X\mathbf w\|_2^2
+
\frac{\lambda}{2}\|\mathbf w\|_2^2.
\]

Take the gradient with respect to $\mathbf w$:

\[
\nabla_{\mathbf w}\mathcal L_{\text{ridge}}(\mathbf w)
=
- \mathbf X^\top(\mathbf y-\mathbf X\mathbf w)
+
\lambda \mathbf w.
\]

Set the gradient equal to zero:

\[
- \mathbf X^\top \mathbf y
+ \mathbf X^\top \mathbf X \mathbf w
+ \lambda \mathbf w
= 0.
\]

Rearrange:

\[
(\mathbf X^\top \mathbf X + \lambda \mathbf I)\mathbf w
=
\mathbf X^\top \mathbf y.
\]

Assuming $(\mathbf X^\top \mathbf X + \lambda \mathbf I)$ is invertible, we obtain

\[
\boxed{
\mathbf w_{\text{ridge}}
=
(\mathbf X^\top \mathbf X + \lambda \mathbf I)^{-1}
\mathbf X^\top \mathbf y.
}
\]

\item SubProblem 3 
By Bayes' rule,
\[
p(\mathbf w\mid \mathbf X,\mathbf y)\ \propto\ p(\mathbf y\mid \mathbf w,\mathbf X)\,p(\mathbf w),
\]
so maximizing the posterior is equivalent to maximizing
$\log p(\mathbf y\mid \mathbf w,\mathbf X)+\log p(\mathbf w)$.

\textbf{Likelihood.}
With $\mathbf y\mid \mathbf w,\mathbf X \sim \mathcal N(\mathbf X\mathbf w,\sigma^2\mathbf I)$,
\[
\log p(\mathbf y\mid \mathbf w,\mathbf X)
=
-\frac{1}{2\sigma^2}\|\mathbf y-\mathbf X\mathbf w\|_2^2 + C.
\]

\textbf{Prior.}
Assume the coordinates are independent and $w_d \sim \mathrm{Laplace}(0,b)$ with
$b=\frac{2\sigma^2}{\lambda}$. Then
\[
p(\mathbf w)=\prod_{d=1}^D \frac{1}{2b}\exp\!\left(-\frac{|w_d|}{b}\right),
\]
hence
\[
\log p(\mathbf w)
=
\sum_{d=1}^D\left[-\log(2b)-\frac{|w_d|}{b}\right]
=
-\frac{1}{b}\sum_{d=1}^D |w_d| + C'
=
-\frac{\lambda}{2\sigma^2}\|\mathbf w\|_1 + C'.
\]

\textbf{Combine.}
Therefore (dropping constants),
\[
\log p(\mathbf y\mid \mathbf w,\mathbf X)+\log p(\mathbf w)
=
-\frac{1}{2\sigma^2}\|\mathbf y-\mathbf X\mathbf w\|_2^2
-\frac{\lambda}{2\sigma^2}\|\mathbf w\|_1
+\text{const}.
\]
Maximizing this is equivalent to minimizing its negative, and multiplying by $\sigma^2$ does not
change the minimizer. Thus,
\[
\arg\max_{\mathbf w} p(\mathbf w\mid \mathbf X,\mathbf y)
=
\arg\min_{\mathbf w}\left(
\frac{1}{2}\|\mathbf y-\mathbf X\mathbf w\|_2^2
+\frac{\lambda}{2}\|\mathbf w\|_1
\right)
=
\arg\min_{\mathbf w}\ \mathcal L_{\text{lasso}}(\mathbf w).
\]

\item SubProblem 4
There is no general closed-form solution for the LASSO estimator because the
$\ell_1$ penalty $\|\mathbf w\|_1 = \sum_d |w_d|$ is not differentiable at $w_d = 0$.
This non-differentiability prevents us from solving the optimization problem by simply setting the gradient equal to zero, as we do in $OLS$ or ridge regression. Unlike the $\ell_2$ penalty in ridge regression, which produces a smooth, quadratic objective with a linear system solution, the $\ell_1$ introduces non-smoothness the objective function. As a result, the minimizer cannot generally be
written in closed form and must instead be found using numerical optimization
methods such as coordinate descent.

\item SubProblem 5
The unregularized basis (d) model massively overfits the training data, making highly oscillatory predictions and a large test MSE. as described earlier, because the 49 cosine basis functions create a very expressive model with high variance.

LASSO regularization adds an $\ell_1$ penalty,
\[
\frac{\lambda}{2}\|w\|_1,
\]
which shrinks coefficients toward zero and effectively reduces model complexity and expressiveness. 

For $\lambda = 1$, the regularization is weak and the model still shows significant variance and overfitting. For $\lambda = 10$, the regularization is stronger, many coefficients are driven closer to zero, the fitted curve becomes smoother, and the test MSE decreases substantially.

Thus, increasing $\lambda$ reduces variance and improves generalization, even though it introduces additional bias.

\begin{figure}[H]
            \centering
            \includegraphics[width=0.6\textwidth]{p4.5.png}
            \end{figure}
	\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Name and Calibration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{Name}: Ibrahim Khaliliya

\textbf{Collaborators and Resources}: Lecture slides 

\end{document}
